<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Eleventy v2.0.1">
    <meta name="author" content="Nibir Sankar">
    <meta name="copyright" content="Nibir Sankar">

    
        <meta name="description" content="">
    

 
    <title>Uncertainties in AI Alignment</title>

    <link rel="alternate" type="application/atom+xml" href="https://nibirsan.org/feed.xml" title="Nibir's Brain Dump"/>
    <link rel="apple-touch-icon" sizes="180x180" href="/media/meta/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/media/meta/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/media/meta/favicon-16x16.png">
    <link rel="manifest" href="/media/meta/site.webmanifest">
    <link rel="mask-icon" href="/media/meta/safari-pinned-tab.svg" color="#2f2f2f">
    <link rel="shortcut icon" href="/media/meta/favicon.ico">
    <meta name="msapplication-TileColor" content="#2b5797">
    <meta name="msapplication-config" content="/media/meta/browserconfig.xml">
    <meta name="theme-color" content="#000000">

    <meta property="og:image" content="/media/meta/og-image.jpg" />
            
    <link rel="stylesheet" href="/css/css.css">

    <link rel="dns-prefetch" href="https://www.googletagmanager.com">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">  
    
</head>
<body>
    <a href="/blog/">..</a>

<article>
  <p class="post-meta">
    <time datetime="Mon Sep 30 2024 08:26:29 GMT+0000 (Coordinated Universal Time)">Sep 30, 2024</time>
  </p>
  
  <h1 class="title">Uncertainties in AI Alignment</h1>
  <h4 class="excerpt"></h4>
  <div class="tag">
<tag>
	
            <a id="technical" href="/blog/t/technical">#technical</a>
		
            <a id="rationality" href="/blog/t/rationality">#rationality</a>
		
</tag> <span class="reading">4 min read</span>
</div>
  <hr>
  
  <p>(Draft) Identifying Uncertainties in AI; AGISF Session 2 exercise.</p>
<h2 id="(possible)-solutions-to-ai-safety" tabindex="-1">(Possible) Solutions to AI Safety <a class="header-anchor" href="#(possible)-solutions-to-ai-safety"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h2>
<h3 id="resolve-human-values" tabindex="-1">Resolve human values <a class="header-anchor" href="#resolve-human-values"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h3>
<p>As I will later talk about this in the post, morality is subjective. Even <em>we</em> humans are not sure of what's right and what's wrong.</p>
<p>Rationalists are fighting against Empiricists. <a href="https://bounded-regret.ghost.io/more-is-different-for-ai/">Engineers against Philosophers</a>. We must align <em>ourselves</em> before aligning AI systems. That means, we have to be more rational, more agreeable to the <em>universal truth</em>; at least the industry leaders and makers.</p>
<h3 id="government-intervention" tabindex="-1">Government intervention <a class="header-anchor" href="#government-intervention"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h3>
<p>It is possible that government intervention: safety rules and regulations, can mitigate the risk of <em>deployment</em> of misaligned AI systems and misuse of AI.</p>
<p>So far, AI developments are outpacing government regulations and law-making. Only the EU seems to <a href="https://www.deloitte.com/nl/en/services/legal/perspectives/the-EU-Artificial-Intelligence-Act-deep-dive.html">have done something</a>.</p>
<h3 id="transparency" tabindex="-1">Transparency <a class="header-anchor" href="#transparency"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h3>
<p>The proprietary AI models lead the market, but that is also a cause of concern. As an open-source endorser, I feel that making the models (and what goes behind) a bit more transparent could make things much better.</p>
<p>I do understand that there are a lot of factors including the economic that make it impossible. But perhaps there can be a <em>communication bridge</em>, at least between companies, that lets them &quot;check and balance&quot; each other's work.</p>
<h3 id="more-resources-in-ai-safety-research" tabindex="-1">More resources in AI safety research <a class="header-anchor" href="#more-resources-in-ai-safety-research"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h3>
<p>Today, the majority of the research being done is in search of AI systems' <em>capabilities</em>, although that seems to be changing. And it should!</p>
<p>We should put in more and and more resources into AI safety research. This will be the most <em>effective</em> measure for a safer future,</p>
<h3 id="more-resources-in-experimental-fields" tabindex="-1">More resources in experimental fields <a class="header-anchor" href="#more-resources-in-experimental-fields"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h3>
<p>Perhaps studying Digital Neuroscience or similar experimental fields can yield good results.</p>
<h2 id="human-level-ai" tabindex="-1">Human-level AI <a class="header-anchor" href="#human-level-ai"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h2>
<p>It is possible for human-level AI to come into existence in the next 10-12 years given the current rate of development. Other forecasters have also predicted that it is possible to get such a AI system before 2040:</p>
<p><picture><source type="image/webp" srcset="/images/human-level-AI-900w.webp 900w, /images/human-level-AI-1495w.webp 1495w" sizes="(min-width: 650px) 1vw, 50vw"><img alt="" loading="lazy" decoding="async" src="/images/human-level-AI-900w.jpeg" width="1495" height="673" srcset="/images/human-level-AI-900w.jpeg 900w, /images/human-level-AI-1495w.jpeg 1495w" sizes="(min-width: 650px) 1vw, 50vw"></picture></p>
<p class="caption"><a href="https://www.metaculus.com/questions/384/humanmachine-intelligence-parity-by-2040/">source</a></p>
<p>The question is if it will be safe or not. because you <em>can</em> make a very intelligent machine by training it with a large dataset and a good optimisation algorithm <em>without</em> putting an effort to align it properly.</p>
<p>Recent developments in AI models, such as OpenAI's o1 model is capable of System-2 thinking, which is <em>one</em> step closer to an AGI.<br>
And which is capable of displaying such feats:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Fucking wild.<a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a>&#39;s new o1 model was tested with a Capture The Flag (CTF) cybersecurity challenge. But the Docker container containing the test was misconfigured, causing the CTF to crash. Instead of giving up, o1 decided to just hack the container to grab the flag inside.… <a href="https://t.co/5FaqQY6bok">pic.twitter.com/5FaqQY6bok</a></p>&mdash; Haseeb ＞|＜ (@hosseeb) <a href="https://twitter.com/hosseeb/status/1834378405896401380?ref_src=twsrc%5Etfw">September 12, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>And that is somewhat <em>concerning</em>, and partly confirms the following prediction</p>
<p><picture><source type="image/webp" srcset="/images/unauth-AI-900w.webp 900w, /images/unauth-AI-1500w.webp 1500w" sizes="(min-width: 650px) 1vw, 50vw"><img alt="" loading="lazy" decoding="async" src="/images/unauth-AI-900w.jpeg" width="1500" height="736" srcset="/images/unauth-AI-900w.jpeg 900w, /images/unauth-AI-1500w.jpeg 1500w" sizes="(min-width: 650px) 1vw, 50vw"></picture></p>
<p class="caption"><a href="https://www.metaculus.com/questions/16014/ai-unauthorized-access-before-2033/">source</a></p>
<p>But in a 100 years... let's hope things don't turn out something like Terminator.</p>
<h2 id="pursuing-convergent-instrumental-goals" tabindex="-1">Pursuing convergent instrumental goals <a class="header-anchor" href="#pursuing-convergent-instrumental-goals"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h2>
<p>Will the AI systems that we build pursue convergent instrumental goals? Maybe.</p>
<p>The model needs to have some sort of situational awareness to be able to do that. So, it is highly probable for &quot;Schemer&quot; AI models to do <a href="https://www.youtube.com/watch?v=Mme2Aya_6Bc">refuse orders</a>, try to get more power, manipulate, etc. all to marginally increase the odds of completing its original goal.</p>
<p>Say, we prompted an AGI to &quot;create a better AI alignment procedure&quot;. It would soon realise that we're aligning AI's to conform to humane ideals, and the easiest way to &quot;align&quot; would be to kill all the humans! (now they'll be all <em>aligned</em> to a null and void) ???</p>
<p>It would be very interesting to see how OpenAI's o1 performs in this regard. Check it's system card <a href="https://openai.com/index/openai-o1-system-card/">here</a>.</p>
<h2 id="safe-by-default" tabindex="-1">Safe by default <a class="header-anchor" href="#safe-by-default"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h2>
<p>Will AI systems we build be safe by default? No, I am not sure. Maybe not in today's AI systems, but perhaps we'll see more safer-by-default systems with time.</p>
<p>Why not today? One of the reasons is our current model training strategy. It is flawed. We can't really understand most of the things, and it makes it really difficult.</p>
<p>Why tomorrow? Because of our efforts in alignment research. Perhaps it will turn out well. But that's only hope, speculation.</p>
<h2 id="one-ai-system%3F-or-more%3F" tabindex="-1">One AI system? or more? <a class="header-anchor" href="#one-ai-system%3F-or-more%3F"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h2>
<p>It depends. It's not hard to imagine the case where AGIs against each other for power (battle royale), make allies, etc. Then we'll have one giant ecosystem or a fleet of <em>one</em> AGI (assuming we're not made slaves or killed beforehand).</p>
<p>But it is also probable that, in a more optimistic future, we'll see lots of different, powerful AI systems, tuned to do different specialised tasks. That might be the case if you <em>limit</em> AI's capabilities.</p>
<h2 id="align-ai-systems-to-what%3F" tabindex="-1">Align AI systems to what? <a class="header-anchor" href="#align-ai-systems-to-what%3F"><span aria-hidden="true" class="header-anchor__symbol">#</span></a></h2>
<p>AI systems should be aligned to human values.</p>
<p>Only the problem is, &quot;human&quot; values are not necessarily &quot;humane&quot;. Morality is subjective (as it has always been). We must know what we mean by morality and alignment in <em>humans</em> before aligning the AI systems to our values. Thus, our statement changes to &quot;AI systems should be aligned to <em>humane</em> values&quot;.</p>
<p>The majority of the AI systems' values and intentions are governed by its stakeholders (which may or may not have good intentions and values). We <em>could</em> say, &quot;safety is more important than money, you selfish prick!&quot; but that might not be very practical. Because one of the many reasons of building AI systems is economic growth and there's a huge market for that. One motivation for people to continue pursuing this market <em>rashly</em> is that &quot;if we don't do it, someone else will. What makes a difference?&quot;.</p>
<p>In that case, there should be government intervention preventing stakeholders to carry inappropriate practices, such as policy-making and a requirement for &quot;safety&quot;.</p>


  <hr>
	<div class="nextread">
		<h3>Continue Reading</h3>
			<p>
			
			
			
					<strong>Previous</strong>:
					<a class="previous" href="/blog/p/thoughts-on-the-prestige-2006/">Thoughts on The Prestige (2006)</a>
			
			</p>
	</div>
	<hr>
	<script src="https://giscus.app/client.js"
        data-repo="moiSentineL/moisentinel.github.io"
        data-repo-id="R_kgDOLKbZeg"
        data-category="Announcements"
        data-category-id="DIC_kwDOLKbZes4Cervi"
        data-mapping="title"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
	</script>	

</article>

    <!-- Google tag (gtag.js) -->
<script src="https://www.googletagmanager.com/gtag/js?id=G-VT53HSCH2H" defer fetchpriority="low"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-VT53HSCH2H');
</script>
    <script src="https://cdn.jsdelivr.net/gh/ncase/nutshell/nutshell.min.js"></script>
<script>
Nutshell.setOptions({
    startOnLoad: true, // Start Nutshell on load? (default: true)
    lang: 'en', // Language (default: 'en', which is English)
    dontEmbedHeadings: true, // If 'true', removes the "embed this as a nutshell" option on headings
});
</script>
</script>
</body>
<footer>
    <hr>
    <div>
        
        <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">
            This domain is licensed under <a href="https://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC 4.0</a>
            <br>
            <a href="https://github.com/moiSentineL/moisentinel.github.io">Source Code <3</a>
        </p>
        
    </div>
    
    <div>
        <p>
            Built with:<br>
        <a href="https://www.11ty.dev/">11ty</a> - static site generator<br>
        <a href="https://pages.github.com/">github pages</a> - site hosting <br>
        <a href="https://ncase.me/nutshell/">nutshell</a> - better explanation snippets <br>
        <a href="https://github.com/markdown-it">markdown-it</a> family for mostly everything.
        </p>
    </div>
</footer>

</html>